{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f67095a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30beff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "177c7240",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in dataset\n",
    "df = pd.read_csv('/Users/taniadawood/Desktop/DSCI550-PixstoryDatasetAnalysisVisualizations/Datasets/Master_Pixstory_Dataset_EXTRACT_ANALYSIS.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "414673f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting dataframe by 'Date (No Timestamp)'\n",
    "df = df.astype(str)\n",
    "df = df.sort_values('Date (No Timestamp)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f40ddf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataframe into subsets of 20k rows each\n",
    "subsets = [df[i:i+20000] for i in range(0, len(df), 20000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccd37c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "my_stopwords = [\"the\", \"i\", \"also\", \"one\", \"it\", \"in\", \"first\", \"said\", \"like\", \"a\", \"would\", \"this\", \"he\", \"made\", \"many\", \"we\", \"well\", \"make\", \"but\", \"even\", \"could\", \"back\", \"get\", \"th\", \"they\", \"i\", \"me\", \"my\", \"myself\", \"we\", \"us\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"whose\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"will\", \"would\", \"should\", \"can\", \"could\", \"ought\", \"I'm\", \"you're\", \"he's\", \"she's\", \"it's\", \"we're\", \"they're\", \"I've\", \"you've\", \"we've\", \"they've\", \"I'd\", \"you'd\", \"he'd\", \"she'd\", \"we'd\", \"they'd\", \"I'll\", \"you'll\", \"he'll\", \"she'll\", \"we'll\", \"they'll\", \"isn't\", \"aren't\", \"wasn't\", \"weren't\", \"hasn't\", \"haven't\", \"hadn't\", \"doesn't\", \"don't\", \"didn't\", \"won't\", \"wouldn't\", \"shan't\", \"shouldn't\", \"can't\", \"cannot\", \"couldn't\", \"mustn't\", \"let's\", \"that's\", \"who's\", \"what's\", \"here's\", \"there's\", \"when's\", \"where's\", \"why's\", \"how's\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"upon\", \"down\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"say\", \"says\", \"said\", \"shall\", \"go\", \"The\", \"I\", \"Also\", \"One\", \"It\", \"In\", \"First\", \"Said\", \"Like\", \"A\", \"Would\", \"This\", \"He\", \"Made\", \"Many\", \"We\", \"Well\", \"Make\", \"But\", \"Even\", \"Could\", \"Back\", \"Get\", \"Th\", \"They\", \"I\", \"Me\", \"My\", \"Myself\", \"We\", \"Us\", \"Our\", \"Ours\", \"Ourselves\", \"You\", \"Your\", \"Yours\", \"Yourself\", \"Yourselves\", \"He\", \"Him\", \"His\", \"Himself\", \"She\", \"Her\", \"Hers\", \"Herself\", \"It\", \"Its\", \"Itself\", \"They\", \"Them\", \"Their\", \"Theirs\", \"Themselves\", \"What\", \"Which\", \"Who\", \"Whom\", \"Whose\", \"This\", \"That\", \"These\", \"Those\", \"Am\", \"Is\", \"Are\", \"Was\", \"Were\", \"Be\", \"Been\", \"Being\", \"Have\", \"Has\", \"Had\", \"Having\", \"Do\", \"Does\", \"Did\", \"Doing\", \"Will\", \"Would\", \"Should\", \"Can\", \"Could\", \"Ought\", \"I'm\", \"You're\", \"He's\", \"She's\", \"It's\", \"We're\", \"They're\", \"I've\", \"You've\", \"We've\", \"They've\", \"I'd\", \"You'd\", \"He'd\", \"She'd\", \"We'd\", \"They'd\", \"I'll\", \"You'll\", \"He'll\", \"She'll\", \"We'll\", \"They'll\", \"Isn't\", \"Aren't\", \"Wasn't\", \"Weren't\", \"Hasn't\", \"Haven't\", \"Hadn't\", \"Doesn't\", \"Don't\", \"Didn't\", \"Won't\", \"Wouldn't\", \"Shan't\", \"Shouldn't\", \"Can't\", \"Cannot\", \"Couldn't\", \"Mustn't\", \"Let's\", \"That's\", \"Who's\", \"What's\", \"Here's\", \"There's\", \"When's\", \"Where's\", \"Why's\", \"How's\", \"A\", \"An\", \"The\", \"And\", \"But\", \"If\", \"Or\", \"Because\", \"As\", \"Until\", \"While\", \"Of\", \"At\", \"By\", \"For\", \"With\", \"About\", \"Against\", \"Between\", \"Into\", \"Through\", \"During\", \"Before\", \"After\", \"Above\", \"Below\", \"To\", \"From\", \"Up\", \"Upon\", \"Down\", \"Out\", \"On\", \"Off\", \"Over\", \"Under\", \"Again\", \"Further\", \"Then\", \"Once\", \"Here\", \"There\", \"When\", \"Where\", \"Why\", \"How\", \"All\", \"Any\", \"Both\", \"Each\", \"Few\", \"More\", \"Most\", \"Other\", \"Some\", \"Such\", \"No\", \"Nor\", \"Not\", \"Only\", \"Own\", \"Same\", \"So\", \"Than\", \"Too\", \"Very\", \"Say\", \"Says\", \"Said\", \"Shall\", \"Go\"]       \n",
    "stopwords.update(my_stopwords)\n",
    "#Grabbing random 3k words from the 'Translated Narrative' column from each subset\n",
    "words = []\n",
    "for subset in subsets:\n",
    "    #Combining all the 'Translated Narrative' columns from the subset into a single string\n",
    "    text = ' '.join(subset['Translated Narrative'].astype(str).tolist())\n",
    "    #Splitting the text into words and count their frequency\n",
    "    word_freq = pd.Series(text.split()).value_counts()\n",
    "    word_freq = word_freq[~word_freq.index.isin(stopwords)]\n",
    "\n",
    "    # Selecting the top 3k most frequent words and add them to the list\n",
    "    for word, count in word_freq.nlargest(3000).items():\n",
    "        words.append({'word': word, 'size': str(math.ceil(count / 100))})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c183f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words[:500])  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
